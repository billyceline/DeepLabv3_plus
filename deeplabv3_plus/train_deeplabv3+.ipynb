{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/opt/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import deeplab_model\n",
    "from utils import preprocessing\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tensorflow.contrib import layers as layers_lib\n",
    "from tensorflow.contrib.slim.nets import resnet_v2\n",
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "from tensorflow.contrib.layers.python.layers import layers\n",
    "from tensorflow.python.framework.graph_util import convert_variables_to_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_CLASSES = 21\n",
    "_HEIGHT = 513\n",
    "_WIDTH = 513\n",
    "_DEPTH = 3\n",
    "_MIN_SCALE = 0.5\n",
    "_MAX_SCALE = 2.0\n",
    "_IGNORE_LABEL = 255\n",
    "\n",
    "_POWER = 0.9\n",
    "_MOMENTUM = 0.9\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.9997\n",
    "\n",
    "_NUM_IMAGES = {\n",
    "    'train': 10582,\n",
    "    'validation': 1449,\n",
    "}\n",
    "\n",
    "_BATCH_NORM_DECAY = 0.9997\n",
    "_WEIGHT_DECAY = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model'\n",
    "clean_model_dir = False\n",
    "train_epochs = 26\n",
    "epochs_per_eval = 1\n",
    "tensorboard_images_max_outputs =6\n",
    "batch_size = 8\n",
    "learning_rate_policy = 'poly'\n",
    "max_iter = 30000\n",
    "data_dir = './dataset/'\n",
    "base_architecture = 'resnet_v2_101'\n",
    "pre_trained_model = './resnet_v2_101/resnet_v2_101.ckpt'\n",
    "output_stride = 16\n",
    "initial_learning_rate = 1e-5\n",
    "end_learning_rate = 1e-6\n",
    "initial_global_step = 0\n",
    "weight_decay = 2e-4\n",
    "debug = False\n",
    "freeze_batch_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, label, is_training):\n",
    "    \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "    if is_training:\n",
    "        # Randomly scale the image and label.\n",
    "        image, label = preprocessing.random_rescale_image_and_label(\n",
    "            image, label, _MIN_SCALE, _MAX_SCALE)\n",
    "\n",
    "        # Randomly crop or pad a [_HEIGHT, _WIDTH] section of the image and label.\n",
    "        image, label = preprocessing.random_crop_or_pad_image_and_label(\n",
    "            image, label, _HEIGHT, _WIDTH, _IGNORE_LABEL)\n",
    "\n",
    "        # Randomly flip the image and label horizontally.\n",
    "        image, label = preprocessing.random_flip_left_right_image_and_label(\n",
    "            image, label)\n",
    "\n",
    "        image.set_shape([_HEIGHT, _WIDTH, 3])\n",
    "        label.set_shape([_HEIGHT, _WIDTH, 1])\n",
    "\n",
    "    image = preprocessing.mean_image_subtraction(image)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "      'output_stride': output_stride,\n",
    "      'batch_size': batch_size,\n",
    "      'base_architecture': base_architecture,\n",
    "      'pre_trained_model': pre_trained_model,\n",
    "      'batch_norm_decay': _BATCH_NORM_DECAY,\n",
    "      'num_classes': _NUM_CLASSES,\n",
    "      'tensorboard_images_max_outputs': tensorboard_images_max_outputs,\n",
    "      'weight_decay': weight_decay,\n",
    "      'learning_rate_policy': learning_rate_policy,\n",
    "      'num_train': _NUM_IMAGES['train'],\n",
    "      'initial_learning_rate': initial_learning_rate,\n",
    "      'max_iter': max_iter,\n",
    "      'end_learning_rate': end_learning_rate,\n",
    "      'power': _POWER,\n",
    "      'momentum': _MOMENTUM,\n",
    "      'freeze_batch_norm': freeze_batch_norm,\n",
    "      'initial_global_step': initial_global_step\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atrous_spatial_pyramid_pooling(inputs, output_stride, batch_norm_decay, is_training, depth=256):\n",
    "  \"\"\"Atrous Spatial Pyramid Pooling.\n",
    "\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, height, width, channels].\n",
    "    output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.\n",
    "      the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n",
    "    batch_norm_decay: The moving average decay when estimating layer activation\n",
    "      statistics in batch normalization.\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    depth: The depth of the ResNet unit output.\n",
    "\n",
    "  Returns:\n",
    "    The atrous spatial pyramid pooling output.\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(\"aspp\"):\n",
    "    if output_stride not in [8, 16]:\n",
    "      raise ValueError('output_stride must be either 8 or 16.')\n",
    "\n",
    "    atrous_rates = [6, 12, 18]\n",
    "    if output_stride == 8:\n",
    "      atrous_rates = [2*rate for rate in atrous_rates]\n",
    "\n",
    "    with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "      with arg_scope([layers.batch_norm], is_training=is_training):\n",
    "        inputs_size = tf.shape(inputs)[1:3]\n",
    "        # (a) one 1x1 convolution and three 3x3 convolutions with rates = (6, 12, 18) when output stride = 16.\n",
    "        # the rates are doubled when output stride = 8.\n",
    "        conv_1x1 = layers_lib.conv2d(inputs, depth, [1, 1], stride=1, scope=\"conv_1x1\")\n",
    "        conv_3x3_1 = layers_lib.conv2d(inputs, depth, [3, 3], stride=1, rate=atrous_rates[0], scope='conv_3x3_1')\n",
    "        conv_3x3_2 = layers_lib.conv2d(inputs, depth, [3, 3], stride=1, rate=atrous_rates[1], scope='conv_3x3_2')\n",
    "        conv_3x3_3 = layers_lib.conv2d(inputs, depth, [3, 3], stride=1, rate=atrous_rates[2], scope='conv_3x3_3')\n",
    "\n",
    "        # (b) the image-level features\n",
    "        with tf.variable_scope(\"image_level_features\"):\n",
    "          # global average pooling\n",
    "          image_level_features = tf.reduce_mean(inputs, [1, 2], name='global_average_pooling', keepdims=True)\n",
    "          # 1x1 convolution with 256 filters( and batch normalization)\n",
    "          image_level_features = layers_lib.conv2d(image_level_features, depth, [1, 1], stride=1, scope='conv_1x1')\n",
    "          # bilinearly upsample features\n",
    "          image_level_features = tf.image.resize_bilinear(image_level_features, inputs_size, name='upsample')\n",
    "\n",
    "        net = tf.concat([conv_1x1, conv_3x3_1, conv_3x3_2, conv_3x3_3, image_level_features], axis=3, name='concat')\n",
    "        net = layers_lib.conv2d(net, depth, [1, 1], stride=1, scope='conv_1x1_concat')\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deeplab_v3_plus_generator(num_classes,\n",
    "                              output_stride,\n",
    "                              base_architecture,\n",
    "                              pre_trained_model,\n",
    "                              batch_norm_decay,\n",
    "                              data_format='channels_last'):\n",
    "    \"\"\"Generator for DeepLab v3 plus models.\n",
    "\n",
    "    Args:\n",
    "    num_classes: The number of possible classes for image classification.\n",
    "    output_stride: The ResNet unit's stride. Determines the rates for atrous convolution.\n",
    "      the rates are (6, 12, 18) when the stride is 16, and doubled when 8.\n",
    "    base_architecture: The architecture of base Resnet building block.\n",
    "    pre_trained_model: The path to the directory that contains pre-trained models.\n",
    "    batch_norm_decay: The moving average decay when estimating layer activation\n",
    "      statistics in batch normalization.\n",
    "    data_format: The input format ('channels_last', 'channels_first', or None).\n",
    "      If set to None, the format is dependent on whether a GPU is available.\n",
    "      Only 'channels_last' is supported currently.\n",
    "\n",
    "    Returns:\n",
    "    The model function that takes in `inputs` and `is_training` and\n",
    "    returns the output tensor of the DeepLab v3 model.\n",
    "    \"\"\"\n",
    "    if data_format is None:\n",
    "    # data_format = (\n",
    "    #     'channels_first' if tf.test.is_built_with_cuda() else 'channels_last')\n",
    "        pass\n",
    "\n",
    "    if batch_norm_decay is None:\n",
    "        batch_norm_decay = _BATCH_NORM_DECAY\n",
    "\n",
    "    if base_architecture not in ['resnet_v2_50', 'resnet_v2_101']:\n",
    "        raise ValueError(\"'base_architrecture' must be either 'resnet_v2_50' or 'resnet_v2_101'.\")\n",
    "\n",
    "    if base_architecture == 'resnet_v2_50':\n",
    "        base_model = resnet_v2.resnet_v2_50\n",
    "    else:\n",
    "        base_model = resnet_v2.resnet_v2_101\n",
    "\n",
    "    def model(inputs, is_training):\n",
    "        \"\"\"Constructs the ResNet model given the inputs.\"\"\"\n",
    "        if data_format == 'channels_first':\n",
    "          # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).\n",
    "          # This provides a large performance boost on GPU. See\n",
    "          # https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "            inputs = tf.transpose(inputs, [0, 3, 1, 2])\n",
    "\n",
    "        # tf.logging.info('net shape: {}'.format(inputs.shape))\n",
    "        # encoder\n",
    "        with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "            logits, end_points = base_model(inputs,\n",
    "                                          num_classes=None,\n",
    "                                          is_training=is_training,\n",
    "                                          global_pool=False,\n",
    "                                          output_stride=output_stride)\n",
    "        ##get resnet pre-trained models from ckpt\n",
    "        if is_training:\n",
    "            exclude = [base_architecture + '/logits', 'global_step']\n",
    "            variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=exclude)\n",
    "            tf.train.init_from_checkpoint(pre_trained_model,\n",
    "                                        {v.name.split(':')[0]: v for v in variables_to_restore})\n",
    "\n",
    "        inputs_size = tf.shape(inputs)[1:3]\n",
    "        net = end_points[base_architecture + '/block4']\n",
    "        encoder_output = atrous_spatial_pyramid_pooling(net, output_stride, batch_norm_decay, is_training)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            with tf.contrib.slim.arg_scope(resnet_v2.resnet_arg_scope(batch_norm_decay=batch_norm_decay)):\n",
    "                with arg_scope([layers.batch_norm], is_training=is_training):\n",
    "                    with tf.variable_scope(\"low_level_features\"):\n",
    "                        low_level_features = end_points[base_architecture + '/block1/unit_3/bottleneck_v2/conv1']\n",
    "                        low_level_features = layers_lib.conv2d(low_level_features, 48,\n",
    "                                                               [1, 1], stride=1, scope='conv_1x1')\n",
    "                        low_level_features_size = tf.shape(low_level_features)[1:3]\n",
    "\n",
    "                    with tf.variable_scope(\"upsampling_logits\"):\n",
    "                        net = tf.image.resize_bilinear(encoder_output, low_level_features_size, name='upsample_1')\n",
    "                        net = tf.concat([net, low_level_features], axis=3, name='concat')\n",
    "                        net = layers_lib.conv2d(net, 256, [3, 3], stride=1, scope='conv_3x3_1')\n",
    "                        net = layers_lib.conv2d(net, 256, [3, 3], stride=1, scope='conv_3x3_2')\n",
    "                        net = layers_lib.conv2d(net, num_classes, [1, 1], activation_fn=None, normalizer_fn=None, scope='conv_1x1')\n",
    "                        logits = tf.image.resize_bilinear(net, inputs_size, name='upsample_2')\n",
    "\n",
    "        return logits\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = deeplab_v3_plus_generator(params['num_classes'],\n",
    "                                      params['output_stride'],\n",
    "                                      params['base_architecture'],\n",
    "                                      params['pre_trained_model'],\n",
    "                                      params['batch_norm_decay'])\n",
    "inputs = tf.placeholder(tf.float32, [None, _WIDTH, _HEIGHT, 3],name='inputs')\n",
    "label = tf.placeholder(tf.int32, [None, _WIDTH, _HEIGHT, 1],)\n",
    "# is_training = tf.placeholder(tf.bool,name='is_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = network(inputs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = tf.expand_dims(tf.argmax(logits, axis=3, output_type=tf.int32), axis=3)\n",
    "inference_result = tf.squeeze(tf.argmax(logits, axis=3, output_type=tf.int32),name='result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.squeeze(label, axis=3)  # reduce the channel dimension.\n",
    "\n",
    "logits_by_num_classes = tf.reshape(logits, [-1, params['num_classes']])\n",
    "labels_flat = tf.reshape(labels, [-1, ])\n",
    "\n",
    "valid_indices = tf.to_int32(labels_flat <= params['num_classes'] - 1)\n",
    "valid_logits = tf.dynamic_partition(logits_by_num_classes, valid_indices, num_partitions=2)[1]\n",
    "valid_labels = tf.dynamic_partition(labels_flat, valid_indices, num_partitions=2)[1]\n",
    "\n",
    "preds_flat = tf.reshape(pred_classes, [-1, ])\n",
    "valid_preds = tf.dynamic_partition(preds_flat, valid_indices, num_partitions=2)[1]\n",
    "confusion_matrix = tf.confusion_matrix(valid_labels, valid_preds, num_classes=params['num_classes'])\n",
    "\n",
    "# predictions['valid_preds'] = valid_preds\n",
    "# predictions['valid_labels'] = valid_labels\n",
    "# predictions['confusion_matrix'] = confusion_matrix\n",
    "\n",
    "cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=valid_logits, labels=valid_labels)\n",
    "\n",
    "\n",
    "if not params['freeze_batch_norm']:\n",
    "    train_var_list = [v for v in tf.trainable_variables()]\n",
    "else:\n",
    "    train_var_list = [v for v in tf.trainable_variables() if 'beta' not in v.name and 'gamma' not in v.name]\n",
    "\n",
    "# Add weight decay to the loss.\n",
    "with tf.variable_scope(\"total_loss\"):\n",
    "    loss = cross_entropy + params.get('weight_decay', _WEIGHT_DECAY) * tf.add_n([tf.nn.l2_loss(v) for v in train_var_list])\n",
    "    \n",
    "optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=initial_learning_rate,\n",
    "        momentum=params['momentum'])\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "# Batch norm requires update ops to be added as a dependency to the train_op\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss, global_step, var_list=train_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list(img_dir,label_dir):\n",
    "    img_list = []\n",
    "    #get the file name from dir 0\n",
    "    for _,file_path,files in os.walk(label_dir):\n",
    "        for file in files:\n",
    "            img_list.append([img_dir+'/'+file.split('.')[0]+'.jpg',label_dir+'/'+file])\n",
    "\n",
    "    random.shuffle(img_list)\n",
    "    return img_list\n",
    "\n",
    "\n",
    "\n",
    "def read_data(img_list,batch_size,aug):\n",
    "    num_batch = len(img_list)/batch_size\n",
    "    count=0\n",
    "    while(True):\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "        for i in range(batch_size):\n",
    "            temp_index = i+count*batch_size\n",
    "            temp_index %=len(img_list) \n",
    "            image = Image.open(img_list[temp_index][0])\n",
    "            image = image.resize((513,513))\n",
    "            image = np.asarray(image, dtype=np.float32)\n",
    "#             image = np.pad(image,((0,513-image.shape[0]),(0,513-image.shape[1]),(0,0)),'constant',constant_values = 0)\n",
    "            image = image/255\n",
    "#             if aug:\n",
    "#                 image = data_augmentation(image)\n",
    "            label = Image.open(img_list[temp_index][1])\n",
    "            label = label.resize((513,513))\n",
    "            label = np.asarray(label, dtype=np.int32)\n",
    "#             label[label>100] = 0\n",
    "#             label = np.pad(label,((0,513-label.shape[0]),(0,513-label.shape[1])),'constant',constant_values = 0)\n",
    "            label = np.expand_dims(label,axis=2)\n",
    "            \n",
    "            x_data.append(image)\n",
    "            y_data.append(label)\n",
    "            \n",
    "        count+=1\n",
    "        x_data = np.array(x_data)\n",
    "        y_data = np.array(y_data)\n",
    "        yield x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if(not os.path.exists('training_list.txt')):\n",
    "#     data_list = create_file_list('/media/xinje/New Volume/VOC07&12/VOC2012/JPEGImages','/media/xinje/New Volume/VOC07&12/VOC2012/SegmentationClass')\n",
    "#     training_list = data_list[0:2500]\n",
    "#     validation_list = data_list[2500::]\n",
    "#     with open('training_list.txt','w') as f:\n",
    "#         for i in training_list:\n",
    "#             f.writelines(i[0]+','+i[1]+'\\n')\n",
    "#     #         f.writelines('\\n')\n",
    "\n",
    "#     with open('validation_list.txt','w') as f:\n",
    "#         for i in validation_list:\n",
    "#             f.writelines(i[0]+','+i[1]+'\\n')\n",
    "# else:\n",
    "#     with open('training_list.txt','r') as f:\n",
    "#         training_list = []\n",
    "#         for i in f.readlines():\n",
    "#             training_list.append(i.strip('\\n').split(','))\n",
    "            \n",
    "#     with open('validation_list.txt','r') as f:\n",
    "#         validation_list = []\n",
    "#         for i in f.readlines():\n",
    "#             validation_list.append(i.strip('\\n').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./training_data/train.txt','r') as f:\n",
    "    training_list = []\n",
    "    for i in f.readlines():\n",
    "        training_list.append(i.strip('\\n').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/Deeplab101-185100\n",
      "restore from the checkpoint ./checkpoints/Deeplab101-185100\n",
      "Global step: 185101  Loss: 21.745855  Cross entropy: 0.0059601013\n",
      "Global step: 185102  Loss: 21.743456  Cross entropy: 0.0035603286\n",
      "Global step: 185103  Loss: 21.744617  Cross entropy: 0.004722013\n",
      "Global step: 185104  Loss: 21.744593  Cross entropy: 0.0046981494\n",
      "Global step: 185105  Loss: 21.746645  Cross entropy: 0.006749954\n",
      "Global step: 185106  Loss: 21.744034  Cross entropy: 0.004139376\n",
      "Global step: 185107  Loss: 21.744352  Cross entropy: 0.0044575655\n",
      "Global step: 185108  Loss: 21.745861  Cross entropy: 0.0059663975\n",
      "Global step: 185109  Loss: 21.745316  Cross entropy: 0.0054199547\n",
      "Global step: 185110  Loss: 21.746656  Cross entropy: 0.006761185\n",
      "Global step: 185111  Loss: 21.748306  Cross entropy: 0.008410799\n",
      "Global step: 185112  Loss: 21.746397  Cross entropy: 0.006501658\n",
      "Global step: 185113  Loss: 21.74414  Cross entropy: 0.004245952\n",
      "Global step: 185114  Loss: 21.744314  Cross entropy: 0.0044201915\n",
      "Global step: 185115  Loss: 21.744106  Cross entropy: 0.004212169\n",
      "Global step: 185116  Loss: 21.744873  Cross entropy: 0.004978159\n",
      "Global step: 185117  Loss: 21.742695  Cross entropy: 0.0027990772\n",
      "Global step: 185118  Loss: 21.744122  Cross entropy: 0.0042265276\n",
      "Global step: 185119  Loss: 21.757452  Cross entropy: 0.0175569\n",
      "Global step: 185120  Loss: 21.743027  Cross entropy: 0.0031322674\n",
      "Global step: 185121  Loss: 21.746683  Cross entropy: 0.0067875544\n",
      "Global step: 185122  Loss: 21.745441  Cross entropy: 0.00554635\n",
      "Global step: 185123  Loss: 21.747475  Cross entropy: 0.0075797453\n",
      "Global step: 185124  Loss: 21.743004  Cross entropy: 0.0031097515\n",
      "Global step: 185125  Loss: 21.745775  Cross entropy: 0.0058802743\n",
      "Global step: 185126  Loss: 21.743572  Cross entropy: 0.0036771896\n",
      "Global step: 185127  Loss: 21.746368  Cross entropy: 0.0064729825\n",
      "Global step: 185128  Loss: 21.74888  Cross entropy: 0.0089857485\n",
      "Global step: 185129  Loss: 21.748148  Cross entropy: 0.0082539115\n",
      "Global step: 185130  Loss: 21.748676  Cross entropy: 0.008781643\n",
      "Global step: 185131  Loss: 21.744808  Cross entropy: 0.0049126637\n",
      "Global step: 185132  Loss: 21.744421  Cross entropy: 0.0045261006\n",
      "Global step: 185133  Loss: 21.744453  Cross entropy: 0.0045580952\n",
      "Global step: 185134  Loss: 21.744764  Cross entropy: 0.0048699277\n",
      "Global step: 185135  Loss: 21.745525  Cross entropy: 0.0056308466\n",
      "Global step: 185136  Loss: 21.745836  Cross entropy: 0.005941494\n",
      "Global step: 185137  Loss: 21.743105  Cross entropy: 0.003210862\n",
      "Global step: 185138  Loss: 21.747225  Cross entropy: 0.0073303785\n",
      "Global step: 185139  Loss: 21.745028  Cross entropy: 0.005132764\n",
      "Global step: 185140  Loss: 21.744432  Cross entropy: 0.0045373193\n",
      "Global step: 185141  Loss: 21.74593  Cross entropy: 0.0060355696\n",
      "Global step: 185142  Loss: 21.74849  Cross entropy: 0.008593791\n",
      "Global step: 185143  Loss: 21.745794  Cross entropy: 0.0058995597\n",
      "Global step: 185144  Loss: 21.744774  Cross entropy: 0.004879341\n",
      "Global step: 185145  Loss: 21.744131  Cross entropy: 0.0042361077\n",
      "Global step: 185146  Loss: 21.74409  Cross entropy: 0.004195125\n",
      "Global step: 185147  Loss: 21.743484  Cross entropy: 0.0035893756\n",
      "Global step: 185148  Loss: 21.743484  Cross entropy: 0.0035887857\n",
      "Global step: 185149  Loss: 21.752886  Cross entropy: 0.012991583\n",
      "Global step: 185150  Loss: 21.745728  Cross entropy: 0.005832677\n",
      "Model saved!!\n",
      "INFO:tensorflow:Froze 585 variables.\n",
      "INFO:tensorflow:Converted 585 variables to const ops.\n",
      "Global step: 185151  Loss: 21.744955  Cross entropy: 0.0050598136\n",
      "Global step: 185152  Loss: 21.745401  Cross entropy: 0.0055056345\n",
      "Global step: 185153  Loss: 21.74643  Cross entropy: 0.006534134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-082fa07d1b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step_np\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtraining_average_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#         validation_images,validation_labels = next(val_iterator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtraining_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1aa2b782dfae>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(img_list, batch_size, aug)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtemp_index\u001b[0m \u001b[0;34m%=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m513\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m513\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#             image = np.pad(image,((0,513-image.shape[0]),(0,513-image.shape[1]),(0,0)),'constant',constant_values = 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    train_iterator = read_data(training_list,8,False)\n",
    "    saver = tf.train.Saver(max_to_keep =5)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.latest_checkpoint('./checkpoints/')\n",
    "    if ckpt:\n",
    "        saver.restore(sess,ckpt)\n",
    "        print('restore from the checkpoint {0}'.format(ckpt))\n",
    "    else:\n",
    "        print('Train deeplabv3 from start!!')\n",
    "        \n",
    "    global_step_np = 0\n",
    "    while(global_step_np<=200000):\n",
    "        training_average_loss = 0\n",
    "        training_images,training_labels = next(train_iterator)\n",
    "#         validation_images,validation_labels = next(val_iterator)\n",
    "        _, global_step_np, train_loss, cross_entropy_temp = sess.run([train_op,global_step, loss,cross_entropy],feed_dict={inputs:training_images,label:training_labels})\n",
    "        print('Global step: '+ str(global_step_np)+'  Loss: '+str(train_loss) +'  Cross entropy: '+str(cross_entropy_temp))\n",
    "        if(global_step_np%50 ==0):\n",
    "            saver.save(sess,os.path.join('./checkpoints','Deeplab101'),global_step = global_step_np)\n",
    "            print('Model saved!!')\n",
    "            #########################################################################################\n",
    "            constant_graph = convert_variables_to_constants(sess, sess.graph_def, ['result'])\n",
    "            with tf.gfile.FastGFile('./model.pb', mode='wb') as f:\n",
    "                f.write(constant_graph.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
